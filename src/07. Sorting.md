# Sorting

## Introduction

Sorting is a very common and useful process. We sort names, salaries, movie grosses, Nielsen ratings, home runs, populations, book sales, to name a few. It is important to understand how sorting works and how it can be done efficiently. By default, we will consider sorting in increasing order:

- For all indices, `i`, `j`: if `i` < `j`, then `A[i] <= A[j]`

Note that we are allowing for duplicates here. For decreasing order, we simply change right side to `A[i] >= A[j]`.

## Simple Sorting Algorithms

### Insertion Sort

The basic idea for insertion sort is to "remove" the items one at a time from the original array and "insert" them into a new array, putting them into the correct sorted order as you insert.

We could accomplish this by using two arrays(as implied above), but that would double our memory requirements. We haven't been worrying much about memory needs (we've been worrying about runtime), but it would be nice to not use more memory than needed. So, we'd rather be able to **sort in place**, allowing us to use only a **constant amount of extra memory**. Space needs can be analyzed similar to runtime.

To implement this, we need to think of this one array being in two parts, a sorted part and an unsorted part:

| SORTED | UNSORTED |
| ------ | -------- |

In each iteration of our outer loop, we will take an item out of the `UNSORTED` section and put it into its correct relative location in the `SORTED` section. How do we pick the item from the `UNSORTED` section? For insertion sort, on iteration `i` of the outer loop, we take element `i` in the array (ignoring sorted/unsorted) and insert it into the correct position in the sorted section of the array.

For example, start with this array:
|`0`|`1`|`2`|`3`|`4`|`5`|`6`|`7`|
|-|-|-|-|-|-|-|-|
|40|70|20|30|50|10|80|60|
For insertion sort, we start by sorting element at `i=0` into the correct location in the sorted array. Since the sorted array is empty and the sorted array grows from the left, this first element is already in its (temporarily) sorted position.
|`0`|`1`|`2`|`3`|`4`|`5`|`6`|`7`|
|-|-|-|-|-|-|-|-|
|**40**|70|20|30|50|10|80|60|
We now move on to `i=1`, and sort `A[1]` into the sorted section of the array. Since `40 < 70`, no values need to switch positions.
|`0`|`1`|`2`|`3`|`4`|`5`|`6`|`7`|
|-|-|-|-|-|-|-|-|
|**40**|**70**|20|30|50|10|80|60|
Next is `i=2`, and sort `A[2]` into the sorted section of the array. Since it is smaller than all of the values in the sorted part of the array, it gets inserted to the front of the sorted array, giving:
|`0`|`1`|`2`|`3`|`4`|`5`|`6`|`7`|
|-|-|-|-|-|-|-|-|
|**20**|**40**|**70**|30|50|10|80|60|
Next is `i=3`, and sort `A[3]` into the sorted section of the array, which is between 20 and 40:
|`0`|`1`|`2`|`3`|`4`|`5`|`6`|`7`|
|-|-|-|-|-|-|-|-|
|**20**|**30**|**40**|**70**|50|10|80|60|
This continues until the entire array has been sorted (i.e. after we finish with `i=7` in the example). Notice that the sorted array grows on each iteration and the unsorted array shrinks. For each iteration, we take the next value in the unsorted array and insert it into the sorted array.

Let's take a look at how this all actually works. Here's the code from a previous version of the textbook:

```Java
public static <T extends Comparable<? super T>> void insertionSort(T[] a, int n)
{ insertionSort(a, 0, n - 1); } // end insertionSort
```

```Java
public static <T extends Comparable<? super T>> void insertionSort(T[] a, int first, int last)
{
	int unsorted, index;
	for (unsorted = first + 1; unsorted <= last; unsorted++)
	{// Assertion: a[first] <= a[first + 1] <= ... <= a[unsorted - 1]
		T firstUnsorted = a[unsorted];
		insertInOrder(firstUnsorted, a, first, unsorted - 1);
	} // end for
} // end insertionSort
```

`insertInOrder(firstUnsorted, a, first, unsorted - 1);` _inserts_ each item in array into its correct spot

```Java
private static <T extends Comparable<? super T>> void insertInOrder(T element, T[] a, int begin, int end)
{
	int index;
	for (index = end; (index >= begin) && (element.compareTo(a[index]) < 0); index--)
	{
		a[index + 1] = a[index]; // make room
	} // end for
	a[index + 1] = element;
} // end insertInOrder
```

- The `for` loop finds the correct spot for `curr` item, going from back to front.

The code is a bit wordy (the authors present it in this way to be more readable). The basic idea though is that the initial method (`insertionSort`) has only array and length as parameters. It calls an overloaded version with start and end index values as parameters, which allows us to sort only part of the array if we want. Each iteration in this method brings one more item from the unsorted portion of the array into the sorted portion. It does this by calling another method to actually move the value into its correct spot. Values are shifted from left to right, leaving a "hole" in the spot where the item should be.

#### Runtime

So, what is the runtime of insertion sort? Using the number of comparisons between array elements as the key instruction, we can calculate the run-time.

Let's try to come up with the number of comparisons needed in the **_worst_** case scenario—**REVERSE SORTED DATA**. This is because, if the data is initially reverse-sorted, each item being moved from the unsorted side of the array must move all the way to the front of the sorted side of the array.

For each interation in the main loop (of `insertionSort`):

- when `unsorted = 1`, $1$ comparison in `insertInOrder` method
- when `unsorted = 2`, $2$ comparison in `insertInOrder` method
- ...
- when `unsorted = N-1`, $N-1$ comparison in `insertInOrder` method

Overall, we get: $1 + 2 + ... + (N-1)=\frac{(N-1)(N)}{2} \implies{O(N^2)}$.

Now, it turns out that on average, the number of comparisons are a bit better, but the average case is still $O(N^2)$.

#### Insertion Sort and Linked Lists

Can insertion sort be used with a linked list?

It turns out that insertion sort is probably more natural with a linked list than with an array. At each iteration, simply remove the front node from the list, and "insert it in order" into a second, new list. In this case, we are not creating _any_ new nodes, just moving the ones we have around.

The worst-case runtime turns out to be $O(N^2)$. See 15.15 in text.

### Selection Sort

Another simple sorting algorithm is selection sort. At iteration `i` of the outer loop, find the `i`th smallest item and swap it into location `i`. So:

- On iteration `i = 0`: find 0th smallest value and swap into location `0`
- On iteration `i = 1`: find 1st smallest value and swap into location `1`
- ...
- On iteration `i = N-1`: find (N-1)th smallest value and swap into location `N-1`

Like insertion sort, selection sort has a very simple implementation using nested for loops (or method calls, as shown in text). We actually saw this algorithm earlier in the term with the [Example2.java](handout/Example2.java)

```Java
public static <T extends Comparable<? super T>>
       void selectionSort(T[] a, int n)
{
    for (int index = 0; index < n - 1; index++)
    {
        int indexOfNextSmallest = getIndexOfSmallest(a,index,n-1);
        swap(a, index, indexOfNextSmallest);
    } // end for
} // end selectionSort
private static <T extends Comparable<? super T>>
        int getIndexOfSmallest(T[] a, int first, int last)
{
    T min = a[first];
    int indexOfMin = first;
    for (int index = first + 1; index <= last; index++)
    {
        if (a[index].compareTo(min) < 0)
        {
            min = a[index];
            indexOfMin = index;
         } // end if
    } // end for
    return indexOfMin;
} // end getIndexOfSmallest

```

#### Run time

So what is the run-time of SelectionSort? Again, we can use comparison as key instruction. However, note the difference between `InsertionSort` and `SelectionSort`. While both have two "for" loops, in `InsertionSort`, "inner" for loop will stop when the item is at the correct insertion point; but in `SelectionSort` we don't do a comparison in the loop header. The iterations in both for loops are based solely on index values:

- In "outer loop", `i` goes from `0` to `N-2`
- In "inner loop", `j` goes from `i+1` to `N-1`
  And we do a comparison in each iteration of the inner loop. Thus it doesn't matter how the data is initially organized. I.e., There is no "worst case" or "best case". **All cases iterate the same number of times and do the same number of comparisons**. This is very different from InsertionSort where the initial data configuration determines the performance

So how many comparisons are needed?

- When `i = 0` inner loop goes from `1` to `N-1`, N-1 comps
- When `i = 1` inner loop goes from `2` to `N-1`, N-2 comps
- When `i = 2` inner loop goes from `3` to `N-1`, N-3 comps
- ...
- When `i = N-2` inner loop goes from `N-1` to `N-1`, 1 comp
  Thus, the total is $1 + 2 + ... + (N-1) = \frac{(N-1)(N)}{2}\implies{O(N^2)}$, the same as InsertionSort worst case.

Note that even though these algorithms are very different they end up with similar analysis and the same result in the worst case. However, clearly they will not always have the same performance, as `InsertionSort` can vary a lot in its run-time

### Bubble Sort

With Bubble Sort, loops through the array and "bubble up" smaller values while "sinking" larger values until each is in its appropriate spot. Item `i` in the array is compared to item `i+1`:

- If the data is sorted, `A[i]` should be less than item `A[i+1]`, in which case we do nothing.
- If `A[i]` is greater than `A[i+1]`, then they are out of order so we swap them.

This continues until the array is sorted. Like insertion sort and selection sort, bubble sort uses two loops. The outer loop executes while the array is unsorted. The inner loop walks through the array and performs the swaps. Below is an example of one iteration of the inner loop.
| 0 | 1 | 2 | 3 | 4 | 5 | 6 |
|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| **50** | **30** | 40 | 70 | 10 | 80 | 20 |
| 30 | **50** | **40** | 70 | 10 | 80 | 20 |
| 30 | 40 | **50** | **70** | 10 | 80 | 20 |
| 30 | 40 | 50 | **70** | **10** | 80 | 20 |
| 30 | 40 | 50 | 10 | **70** | **80** | 20 |
| 30 | 40 | 50 | 10 | 70 | **80** | **20** |
| 30 | 40 | 50 | 10 | 70 | 20 | **80** |

#### Run-time

`BubbleSort` is also $O(N^2)$ in the worst case.
(Analysis is omitted)

Bubble sort is famous for its inefficiency. Donald Knuth (famous computer scientist), in the 3rd volume of his book _The Art of Computer Programming_, said that "bubble sort seems to have nothing to recommend it". [Wikipedia's article on Bubble Sort](https://en.wikipedia.org/wiki/Bubble_sort) often points out how it is a bad choice for sorting. However, it is not the worst sorting algorithm. That award goes to [Bogosort](https://en.wikipedia.org/wiki/Bogosort) or a related algorithm. Basically, don't use Bubble Sort. We cover it hear just so you've seen it before.

### Ending Comments on Simple Sorting Algorithms

The text also discusses recursive implementations of InsertionSort and SelectionSort. As with Sequential Search and some other simple problems, this is more to show how it _can_ be done rather than how it should be done (recall the recursion overhead and added implementation complexity with recursion). Read over these explanations and convince yourselves that the recursive versions do the same thing as the iterative versions.

All three of the sorting algorithms we've looked at have similar runtimes in the worst case: $O(N^2)$. However, `InsertionSort` is actually a good choice for **mostly-sorted arrays**. `SelectionSort` is a good choice when **swapping values is expensive**. `BubbleSort` is **never a good choice**. The runtime of $O(N^2)$ makes these algorithms ok for small arrays, but for a large number of items, it is too big. What we'll look at next is how to come up with faster sorting algorithms.

End of Lecture 17

---

### Shellsort

To improve on our simple sorts it helps to consider why they are not so good. Let's again consider `InsertionSort`. What about the algorithm makes its performance poor? Consider what occurs with each comparison:

- Either nothing (if items are relatively in order)
- Or a data move of 1 location (i.e. it only moves a small amount)

If the data is greatly out of order, it will take a lot of comparisons to get into order. If we can move the data farther with one comparison, perhaps we can improve our runtime. This is the idea of **Shellsort**. Rather than comparing adjacent items, we compare items that are farther away from each other. Specifically, we compare and "sort" items that are _K_ locations apart for some _K_. That is, we apply Insertion Sort to non-contiguous subarrays of our original array that are K locations apart. We gradually reduce K from a large value to a small one, ending with K = 1 (which is straight insertion sort). Let's take a look at an example:

We want to sort this array:
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 40 | 20 | 70 | 60 | 50 | 10 | 80 | 30 |
For our first iteration, K = 4.
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| **40** | _20_ | 70 | **_60_** | **50** | _10_ | 80 | **_30_** |

After this first iteration, each sub-array has had insertion sort applied to it, giving this array:
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| **40** | _10_ | 70 | **_30_** | **50** | _20_ | 80 | **_60_** |

We now move on to the next iteration of shellsort. This time, K=2, giving us new sub-arrays. Starting with the partially-sorted array from the first iteration, our sub-arrays are now:
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| **40** | _10_ | **70** | _30_ | **50** | _20_ | **80** | _60_ |
Applying insertion sort to these two sub-arrays, we end up with:
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| **40** | _10_ | **50** | _20_ | **70** | _30_ | **80** | _60_ |
Finally, on the next iteration, K=1, giving us just a single sub-array:
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 40 | 10 | 70 | 30 | 50 | 20 | 80 | 60 |
The idea is that by the time K = 1, most of the data will not have very far left to move.

Applying insertion sort to this sub-array, yields:
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 10 | 20 | 30 | 40 | 50 | 60 | 70 | 80 |

It seems like this algorithm will actually be worse than Insertionsort - why? It's last "iteration" is a full Insertion Sort and previous iterations do Insertion Sorts of sub-arrays. However, when timed it actually outperforms Insertion Sort. The exact analysis is tricky, and depends on the initial value for K. The basic idea is that Shellsort moves the data closer to Insertion Sort's best case arrangement. While best-case analysis is often ignored, if you look at Insertion Sort's best-case performance, it's $O(N)$. So, Shellsort can benefit from this towards the end of its sorting. A good implementation will have about $O(N^{3/2})$ performance compared to $O(N^2)$ for regular Insertion Sort. See the text for more details.

You may wonder **how K is picked**. This is actually a [complex question with ongoing research](https://en.wikipedia.org/wiki/Shellsort#Gap_sequences) and beyond the scope of this course. In the example above (and the code below), we started with $K = \verb|floor|(N / 2)$ and updated $K$ by dividing it by 2 and taking the floor. This is easy to understand, but actually yields $O(Na62)$. More advanced classes would go over this analysis, except there are faster sorting algorithms with interesting analysis opportunities to focus on instead.

The code for Shellsort is:

```Java
public static <T extends Comparable<? super T>>
       void shellSort(T[] a, int first, int last)
{
 	int n = last - first + 1; // number of array elements
	int space = n / 2;	// initial gap is n/2

	// Continue until the gap is zero
	while (space > 0)
	{
		// Insertionsort all of the subarrays determined by
		// the current gap
 	     for (int begin = first; begin < first + space; begin++)
		{
	     	incrementalInsertionSort(a, begin, last, space);
		}

		space = space / 2;	// reduce gap
 	} // end for
} // end shellSort
```

```Java
private static <T extends Comparable<? super T>>
	void incrementalInsertionSort(T[] a, int first,
						int last, int space)
{
	int unsorted, index;
	for (unsorted = first+space; unsorted<=last;
					unsorted=unsorted+space)
	{
		T nextToInsert = a[unsorted];
		index = unsorted – space;
		while ((index >= first) &&
				mextToInsert.compareTo(a[index] < 0))
		{
			a[index + space] = a[index];
			index = index – space;
		} // end while

		a[index + space] = nextToInsert;
	} // end for
} // end incrementalInsertionSort
```

## Faster Sorting Algorithms

If we approach sorting in a different way, we can improve the run-time even more. Let's take a look at divide and conquer approaches. The general idea is to define sorting an array of N items in terms of sorting one or more smaller arrays (for example, of size N/2). As we said previously (for Binary Search), this works well when implemented using recursion. So we will look at the next two sorting algorithms recursively.

How can we apply divide and conquer to sorting? There are two questions to consider:

1. How do we "divide" the problem into sub-problems?
   - Do we break the array in half, or in some other fragment?
   - Do we break it up by index value, or in some other way?
2. How do we use the solutions of the sub-problems to determine the overall solution?
   - Once our recursive call(s) complete, what more needs to be done (if anything) to complete the sort?

### Merge Sort

Merge sort is a divide and conquer sorting algorithm. Let's examine the questions above for Merge sort:

How do we "divide" the problem into sub-problems? Simply break the array in half based on index value. Given the array:
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 50 | 80 | 60 | 20 | 30 | 10 | 70 | 40 |

We would divide it into these two arrays:
| 0 | 1 | 2 | 3 |
|:--:|:--:|:--:|:--:|
|50|80|60|20|
and
|4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|
| 30 | 10 | 70 | 40 |

We then recursively divide each array in two, giving:
|0|1|
|-|-|
|50|80|

| 2   | 3   |
| --- | --- |
| 60  | 20  |

| 4   | 5   |
| --- | --- |
| 30  | 10  |

| 6   | 7   |
| --- | --- |
| 70  | 40  |

Then recursively divide each array in two again and again and again until we reach our base case. In the example array, dividing in two again gives:

`0` - `[50]`
`1` - `[80]`
`2` - `[60]`
`3` - `[20]`
`4` - `[30]`
`5` - `[10]`
`6` - `[70]`
`7` - `[40]`
This is our base case (where all arrays are single-element arrays). When implementing Merge sort, you often don't actually create new arrays when making recursive calls. Instead, you pass in the full array, and both the starting index and ending index into your recursive calls. This is why the arrays above still have their original indices.

Once you reach your base case, you need to decide how to sort a single-element array.

Now with your single-element array sorted, we now need to "pick up the pieces and put them together again", which brings us to our second question: How do we use sub-problem solutions to solve the overall problem?

When the recursive calls complete, we will have two sorted sub-arrays, one on the left and one on the right. Let's look at this from the first call's point of view (this is after the two recursive calls have completed - that's why they're each sorted):

| `0` | `1` | `2` | `3` |
| --- | --- | --- | --- |
| 20  | 50  | 60  | 80  |

| `4` | `5` | `6` | `7` |
| --- | --- | --- | --- |
| 10  | 30  | 40  | 70  |

How do we produce a single sorted array from these two sorted subarrays? We "merge" them together, moving the next appropriate item into an overall sorted array. Note that this is where we are really doing the "work" of the sort. We are comparing items and moving them based on those comparisons.
![[mergeSort-merge.png]]
Let's look at a pseudocode implementation:

```
MergeSort(A)
    if (size of A > 1)
        Break A into left and right halves
        Recursively sort left half
        Recursively sort right half
        Merge sorted halves together
```

Looking at the pseudocode, the algorithm seems pretty easy. The algorithm doesn't return the sorted array because the sorted array is stored in A. The only part that requires some thought is the merge.

```Java
private static <T extends Comparable<? super T>>
        void merge(T[] a, T[] tempArray, int first, int mid, int last)
{
     int beginHalf1 = first;     int endHalf1 = mid;
     int beginHalf2 = mid + 1;   int endHalf2 = last;

     int index = beginHalf1;
     for (; (beginHalf1 <= endHalf1) && (beginHalf2 <= endHalf2); index++)
     {
          if (a[beginHalf1].compareTo(a[beginHalf2]) <= 0)
          {
               tempArray[index] = a[beginHalf1];
               beginHalf1++;
          }
          else
          {
               tempArray[index] = a[beginHalf2];
               beginHalf2++;
          }  // end if
     }  // end for
     for (; beginHalf1 <= endHalf1; beginHalf1++, index++)
          tempArray[index] = a[beginHalf1];

     for (; beginHalf2 <= endHalf2; beginHalf2++, index++)
	 tempArray[index] = a[beginHalf2];

     for (index = first; index <= last; index++)
          a[index] = tempArray[index];
}  // end merge
```

Note that MergeSort does not sort in place. We merge into a temp array and then copy back, which adds overhead (linear) to the run-time.
Additionally, MergeSort is recursive and has many recursive calls.
We don't want to make a new temp array at each call – this would add much more overhead. Rather we make one temp array and then pass it into the recursive method:

EOL

---

#### Runtime

How long does MergeSort take to run? Consider an original array of size N. The analysis is tricky due to the recursive calls, so let's think of the work "level by level". At each level of the recursion, we need to consider and possibly move $O(N)$ items. Since the size is cut in half with each call, we have a total of $O(\log_2(N))$ levels. Thus, we have $N * \log_2(N)$ work to do, so our runtime is $O(N \log_2(N))$. Note that when multiplying Big-O terms, we do **_not_** throw out the smaller terms.

Formal run-time analysis can be done for MergeSort using recurrence relations.  We will do it informally here.  Note that at each level we are doubling the number of calls but we are cutting their sizes in half.  Thus we have (approximately) the same amount of work to do at each level.

We are looking at MergeSort "level by level" simply to do the analysis. The actual execution of MergeSort is a tree execution, similar to what we did for Hanoi. We recursively sort the left side of the array, going all the way down to the base case, then merging back up, before we even start working on the right side.
![[mergesort-tree.png]]
Yet we know Towers of Hanoi required $2^N-1$ moves while MergeSort only requires $O(N\lg{N})$ comparisons. This is because the Towers of Hanoi decreases the problem by N-1, while MergeSort decreases the problem by $\frac{N}{2}$.

##### Slightly more formal MergeSort asymptotic analysis

Execution Trace has approx. $\log_2{N}$ levels
Each call of size $X$ spawns 2 calls of size $X/2$
At each call of size $X$ we have to do $X$ work (merge)
This leads to a sum of$$N + 2(N/2) + 4(N/4) + …$$
If we assume $N = 2^K$ for some $K$, this leads to $$2^02^K + 2^12^{K-1} + 2^22^{K-2} + … + 2^K2^0 = (K+1)2^K$$But we know $K = \log_2{N}$ so we get$$(\log_2{N+1})N \implies O(N\log_2N)$$
Compare this to Binary Search – we still have $\log{N}$ levels but instead of 1 operation per level, we have $N$ operations per level.

While the runtime of $O(N \log_2(N))$ is a definite improvement over the simple sorting algorithms with $O(N^2)$, it comes at a cost. We **are not** sorting in place: the merge step requires an auxiliary array, giving us O(N) additional space required. While that's not much for today's computers, copying to/from this auxiliary array slows down the algorithm in real terms (i.e. timing the algorithm shows that it's slower than it should be).

### Quick Sort

Quick sort is a divide and conquer algorithm that makes a few different decisions than Merge Sort.

When dividing the problem into smaller sub-problems, Quick Sort breaks up the data based on how it compares to a special data value (called the **pivot**). We compare all values to this pivot and place them into three groups:

| `data <= pivot` | `pivot` | `data >= pivot` |
| --------------- | ------- | --------------- |

Since we are dividing the data by comparing values to a value from the data, the division may **not** be exactly half. Let's take a look at an example (same as Merge Sort's):
| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 50 | 80 | 60 | 20 | 30 | 10 | 70 | 40 |

Before we can divide, we need to pick a pivot. For now, let's just take the last value of the array: `A[last]` (in this case, `A[7]`, or `50`). Note that after the "divide" step, the pivot may end up at a different index since it'll be between the two groups formed during division.

With our pivot picked, we now divide the data (called "**partition**"). We'll see later how this partitioning happens, but for now we'll just focus on understanding what Quick Sort does.
![QS Partition](QuickSort-Partition.png)
So, what does this achieve? The data isn't sorted yet, but at least we know the final position of one value(`pivot`) and the others are "more sorted" than they were before, since it is at least on the correct "side" of the array.

Naturally, the "divide" is not complete without recursive calls. For QuickSort, we can now recursively sort the left "side" and the right "side", noting that these side may not be exactly ½ of the array.

We're now ready for some pseudocode:

```
QuickSort(A)
    if (size of A > 1)
        Choose a pivot
        Partition A into left and right sides based on pivot
	        Recursively sort left side
	        Recursively sort right side
```

How do we use the sub-problem solution to solve the overall problem? We don't need to do anything! During the partition step, we move the pivot to its final position. Then, the two recursive calls sort the left and right sides. Once those are done, there's no work to convert those sub-problem solutions into the solution. So even though we need to consider this question, it turns out that for Quick Sort we don't need to actually do anything (unlike MergeSort).

Now we just need to figure out the partition step. Well, it'd be nice to do this in-place so we don't need any extra memory. The basic idea for partitioning is:

1. Start with a counter on the left of the array and a counter on the right of the array
2. As long as data at left counter is less than the pivot, do nothing (just increment counter)
3. As long as the data at right counter is greater than the pivot, do nothing (just decr. counter)
   - Idea here is that data is already on the correct side, so we don't have to move it
4. When left counter and right counter "get stuck", it means there is data on the left that should be on the right, and vice versa
   - So swap the values and continue

```Java
public static <T extends Comparable<? super T>>
			void quickSort(T[] array, int n)
{
	quickSort(array, 0, n-1);
}

public static <T extends Comparable<? super T>>
		   	void quickSort(T[] array, int first, int last)
{
	if (first < last)
	{
		int pivotIndex = partition(array, first, last);

		quickSort(array, first, pivotIndex-1);
		quickSort(array, pivotIndex+1, last);
	}
}

```

```Java
private static <T extends Comparable<? super T>> int partition(T[] a, int first, int last)
{
    int pivotIndex = last;  // simply pick pivot as rightmost element
    T pivot = a[pivotIndex];
    int indexFromLeft = first;
    int indexFromRight = last - 1;

    boolean done = false;
    while (!done)
    {
	while (a[indexFromLeft].compareTo(pivot) < 0)
	    indexFromLeft++;

	while (a[indexFromRight].compareTo(pivot) > 0 && indexFromRight > first)
	    indexFromRight--;

	if (indexFromLeft < indexFromRight)
	{
	    swap(a, indexFromLeft, indexFromRight);
	    indexFromLeft++;
	    indexFromRight--;
	}
	else
	    done = true;
    } // end while
    swap(a, pivotIndex, indexFromLeft);
    pivotIndex = indexFromLeft;
    return pivotIndex;
}  // end partition


```

For a complete implementation of Quick Sort, see [Quick.java](handout/Quick.java).

#### Runtime

How long does QuickSort take to run? The performance of QuickSort depends on the "quality" of the partitions (i.e. how other values relate to the pivots). Let's look at two different scenarios:

1. Pivot is always the **middle value** in a partition.
   |`< pivot` |`pivot`|`>pivot`|
   |-------|-----|-------| - This execution trace is similar to that of MergeSort, and the overall Big-O runtime is also $O(N\cdot \log_2(N))$. However, since an extra array is NOT needed in QuickSort, the measured runtime will usually be faster than MergeSort.
2. Pivot is **always** an **extreme element** in a partition. Note that this is not the index of the pivot, but rather where the pivot ends up after the partition is complete.
   | `< pivot` | `pivot`|
   |--------|-----| - Let's take a look at how this affects the runtime. Recall the idea of divide and conquer: Recursive calls are a fraction of original size (ex: 1⁄2). However, in this case the recursive calls are only one smaller than the original size ($N-1$). Thus we are losing the power of divide and conquer in this case. Run-time ends up being $O(N^2)$. - Why? 1. In first call partition must compare $N-1$ items to the pivot 2. In second call partition must compare $N-2$ items to the pivot - note in this case each call only spawns 1, not 2 recursive calls 3. In third call partition must compare $N-3$ items to the pivot 4. … 5. Resulting sum is $1 + 2 + … + N-1 = (N-1)(N)/2 \implies O(N^2)$

So which run-time will we actually get? It depends on how the data is originally distributed and how the pivot is chosen. Our simple version of Quicksort picks `A[last]` as the pivot. This makes the interesting **worst case of the data being already sorted**! The pivot is always the greatest element and there is no data in the "greater than the pivot" partition. **Reverse sorted data is also a worst case** (now the pivot is always the smallest item).

However, for “random” data it should perform well since it is not likely that poor pivots will consistently be chosen. In the average / expected case, our simple QuickSort has an $O(N\lg_2{N})$ run-time. Nonetheless, we don't like the relatively simple scenario that leads to a worst case behavior.

EOL 19

---

#### Optimizations

##### Optimization: Median of Three

We can make the worst case less likely to occur by choosing the pivot in a more intelligent way. One technique is the **Median of Three**. With this technique, don't pick the pivot from any one index. Rather, consider three possibilities each time we partition: `A[first]`, `A[mid]`, `A[last]`. Order these items and put the smallest value back into `A[first]`, the middle into `A[mid]` and the largest into `A[last]`. So now we know that `A[first]` `<=` `A[mid]` `<=` `A[last]`. Now use `A[mid]` as the pivot—it is the median of the 3 values that we checked.

```Java
private static <T extends Comparable<? super T>>
	        int partition(T[] a, int first, int last)
{
    int mid = (first + last)/2;
    sortFirstMiddleLast(a, first, mid, last);  // calc. med. of 3
    swap(a, mid, last - 1);  // swap pivot into 2nd last location (why?)

    int pivotIndex = last - 1;  // pivot is 1 over from last
    T pivot = a[pivotIndex];
    int indexFromLeft = first + 1;
    int indexFromRight = last - 2;   // start two over from last

    boolean done = false;
    while (!done)
    {
        while (a[indexFromLeft].compareTo(pivot) < 0)
	   indexFromLeft++;
        while (a[indexFromRight].compareTo(pivot) > 0) // compare to
	   indexFromRight--;			   // simple partition

        // rest of code is the same as the simple version of partition
} // end partition
```

How does this affect the runtime? Now, sorted data is the best case! However, the median of three does not guarantee that the worst case ($N^2$) will not occur. It only reduces the likelihood and makes the situation in which it would occur not obvious. So we say that the **_expected_** runtime of QuickSort is $O(N \log_2(N))$, but the **_worst case_** runtime of QuickSort is $O(N^2)$. For code, see [TextMergeQuick.java](handout/TextMergeQuick.java). Note that this is both true for simple pivot and median of three.

##### Optimization: Random Pivot

What if we choose the pivot index randomly? For each call, choose a random index between first and last (inclusive) and use that as the pivot. The worst case could be just as bad as the simple pivot choice. But in general, it is very unlikely that a random pivot will always be bad.

Let $P(i):=$ the probability that `i`th pivot is extreme.$$P(1)=\frac{2}{n}$$$$P(2)=\frac{2}{(N-1)}$$$$P(3)=\frac{2}{(N-2)}$$$$...$$$$\verb|Total |P=\prod_i{P(i)}=\frac{2^N}{N!}$$ which is very small. Thus, the probability of worst case is very small.

Therefore, overall, this should give good results($O(N\log_2{N}$). However, we have overhead of generating random numbers; and we must time it to see the actual performance.

##### Optimization: Dual Pivot

The idea behind dual pivot is to use two pivots $P_1$ and $P_2$ and create three partitions:

1. Items that are < $P_1$
2. Items that are >= $P_1$ and <= $P_2$
3. Items that are > $P_2$
   This yields three subarrays that must be sorted recursively. As long as the pivots are chosen wisely, this actually has an incremental improvement over traditional QuickSort. In fact, [dual pivot has been incorporated into Java 7's JDK](http://permalink.gmane.org/gmane.comp.java.openjdk.core-libs.devel/2628). See: [Reference](<[http://codeblab.com/wp-content/uploads/2009/09/DualPivotQuicksort.pdf](http://codeblab.com/wp-content/uploads/2009/09/DualPivotQuicksort.pdf)>)

##### Optimization: Stopping Recursion Early

Simple QuickSort stops when the logical size of the array is 1. However, the benefit of divide in conquer decreases as the problem size gets smaller. At some point, the cost of the recursion outweighs the divide and conquer savings. So, choose a size > 1 to stop recursing and switch to another (good) algorithm at that point.

What good sorting algorithm should we pick? Insertion Sort. Even though it is poor overall, if the data is "mostly" sorted due to QuickSort, we will be close to the best case for Insertion Sort and maybe we will get better overall results! See [TextMergeQuick.java](handout/TextMergeQuick.java).

```Java
public static <T extends Comparable<? super T>>
	       void quickSort(T[] a, int first, int last)
{
	if (last - first + 1 < MIN_SIZE)
	{
	    insertionSort(a, first, last);
	}
	else
	{
	    int pivotIndex = partition(a, first, last);

	    quickSort(a, first, pivotIndex - 1);
	    quickSort(a, pivotIndex + 1, last);
	} // end if
}

```

In this code, `MIN_SIZE` can be determined empirically.

### Quick Sort vs. Merge Sort

So which do we prefer, Merge Sort or Quick Sort?

- Merge Sort has a more consistent runtime than Quick Sort
- However, in the normal case, Quick Sort out-performs MergeSort
  - Due to the extra array and copying of data, Merge Sort is "normally" slower than Quick Sort
    - This is why many predefined sorts in programming languages are actually Quick Sort (e.g. [Arrays.sort](http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#sort%28byte[]%29) for primitive types)
- However, Quick Sort is not a stable sort
  - Given two equal items, $X_1$ and $X_2$ where $X_1$ appears before $X_2$ in the original data:
    - After Merge Sort, $X_1$ will still be before $X_2$
    - This is not guaranteed in Quick Sort
  - For example:
    - Amy Arnoldson has a salary of 100000
    - Mort Mortenson also has a salary of 100000
    - Assume with a lot of other people the data is sorted **first alphabetically** and **then by salary**
    - So from the second sort point of view Amy and Mort are equal (because they have the same salaries
    - With a stable sort Amy should appear before Mort but with an unstable sort they could be reversed

Thus, for complex (Object) types, it may be better to use `MergeSort` even if it is a bit slower. For example, JDK 6 Java used `MergeSort` for objects and `QuickSort` for primitive types. Since stability does not matter for primitive types, they picked the faster algorithm. For objects, where stability could be important, they chose the less-fast (but definitely not slow) sorting algorithm. However, in JDK 7 they switched to [TimSort](http://en.wikipedia.org/wiki/Timsort) which is **much** more complicated but a bit faster.

Why is MergeSort stable?

```Java
Why is MergeSort stable?
Note the relevant code in merge():

for (; (beginHalf1 <= endHalf1) && (beginHalf2 <= endHalf2); 							   index++)
{
    if (a[beginHalf1].compareTo(a[beginHalf2]) <= 0) // Ref. A
    {
	 tempArray[index] = a[beginHalf1];
	 beginHalf1++;
    }
    else
    {
	 tempArray[index] = a[beginHalf2];
	 beginHalf2++;
    }
}
```

In Line `Ref. A`, we are using `<` instead of `<=`. If it where `<=` an equal item would be taken from the right side ratehr than the left, causing it not to be stable.

#### What about sorting a linked list?

Mergesort works but there is more overhead, because it takes $O(N)$ to divide a linked list in half(it is constant with an array). However, it does not add any extra asymptotic work. The work required in each “call” for the Merge method is also $O(N)$. Splitting just also takes $O(N)$ for the linked list([LListWithSort.java](handout/LListWithSort.java)). Quicksort also could work but, it would require doubly linked list. Thus the partition overhead would be more than is worthwhile.

### RadixSort

We've looked at comparison based sorts, which sorted data by comparing elements to other elements and moving them if necessary; and we've determined that a lower bound on comparison based sorts is $N\lg{N}$. Can we do better if we use a (very) different approach to sorting?

Consider an array of `String`. We can use a comparison based sort to sort these, utilizing the `compareTo()` method. However, if we recognize that a `String` is an **array of characters**, perhaps we can take another approach:

- Consider the positions in each String, from rightmost to leftmost, and the character value at that position
- Instead of comparing these characters to one another, we will use each as an index to a "bin" (actually a Queue) of Strings
  - We will have an array of Queues indexed on the ASCII characters

![[RadixSort-Example-1.png]]
![[RadixSort-Example-2.png]]
![[RadixSort-Example-3.png]]![[RadixSort-Example-4.png]]![[RadixSort-Example-5.png]]
![[RadixSort-Example-6.png]]
What if the Strings are of different lengths? Recall that we start at the **right index of the Strings**(position `length()-1`). If Strings have different lengths then we will get different indicies for different Strings.

Consider `A[]=[HELP, HELPS, HELPED]`. Clearly, `A[0].length() == 4`, `A[1].length() == 5` and `A[2].length() == 6` and will yield different indicies.

How can we handle this situation? We can "pad" the smaller Strings to make sure that they are all the same length. Since alphabetically, we would expect `HELP < HELPED < HELPS`, we would pad the right side. We can get the sort to work correctly if we pad on the right of smaller Strings with characters that are less than any valid characters in a word:
`A[0] = HELP@@`, `A[1] = HELPS@`, `A[2] = HELPED`

```Java
public class RadixDemo
{
public static int numValues = 27;
public static void RadixSort(String [] data)
{
	ArrayList<Queue<String>> bins;
	if (data.length > 0)
	{
	    int max = data[0].length();     // find max String length
	    data[0] = data[0].toUpperCase();
	    for (int i = 1; i < data.length; i++)
	    {
		if (data[i].length() > max) max = data[i].length();
		data[i] = data[i].toUpperCase();   // make upper case
	    }

	    bins = new ArrayList<Queue<String>>();   // make Queues
	    for (int i = 0; i < numValues; i++)
		bins.add(new LinkedList<String>());
	    char curr;
	    for (int k = max-1; k >= 0; k--)  // Go through all positions
										 //(backward)
	    {
		    for (int i = 0; i < data.length; i++) // Process all N
											     // strings
		    {
				if (data[i].length()-1 < k)
				    curr = '@';  // No char so use '@'
							    //(char just before 'A')
				else
				    curr = data[i].charAt(k);

				int loc = curr - '@';	 // map char into index
				bins.get(loc).offer(data[i]);	 // put String into Queue
		    }
		    int count = 0;
		    for (int j = 0; j < bins.size(); j++) // put data back into
												 // array
		    {
				while (!bins.get(j).isEmpty())
				{
				    data[count] = bins.get(j).poll();
				    count++;
				}
		    }
		}
	}
}
```

#### Run-time

We must iterate through each position in a `String`. - For each position we must iterate through all of the Strings, putting each into a bucket - We must then remove them from the buckets and put them back into the array
If the max String length is $K$, and the length of the array is $N$, this will yield a run-time of $O(KN)$. If we consider $K$ _to be a constant_ then this run-time will be $O(N).$

However, that there is considerable overhead here.

- Space overhead for the bins ($O(N)$)
- Time overhead for the copying (_not in place_)
  Also, overhead in extracting the individual values - For `String` this is not a problem - However, consider `RadixSort` of an `int`. Isolating each digit requires some math (i.e. overhead)
  Also, even though this is $KN$ vs. $N\lg N$ for comparison based sort, the value of $K$ may be larger than $\lg N$ for small or medium sized arrays
- Ex: Sorting 1000 Strings of maximum length 15 requires 15xN work for Radix Sort while in this case lgN is only approx. 10.

#### Pitfalls of RadixSort

Radix Sort is not a generally applicable sorting algorithm, because we must be able to break our key into separate values for which the ordering can be utilized. On the other hand, comparison-based sorts allow for arbitrary algorithms to be used for the comparison – perhaps even utilizing multiple data values. Still in some situations it could be effective, and it also enables us to look at sorting in a different way.
