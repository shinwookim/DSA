# Searching
## Introduction
Consider the task of searching for an item within a collection.
	E.g., Given some collection $C$ and some key value $K$, find/retrieve the object whose key matches $K$.

So far, we know how to search:
1. in an Unsorted Array/Vector
	- Algorithm: Sequential Search
	- Run-time: $\theta(N)$
2. in a Sorted Array/Vector
	- Algorithm: Binary Search
	- Run-time: $\theta({\log_2{N}})$
3. in an Unsorted Linked List
	- Algorithm: Sequential Search
	- Run-time: $\theta(N)$
4. in an Sorted Linked List
	- Algorithm: Sequential Search
	- Run-time: $\theta(N)$

So right now, the techniques we have looked at are doing direct comparisons of the keys, and our best time for searching came to be $\theta(\log_2N)$. Could we possibly do any better? Perhaps if we use a very different approach.

## Symbol Tables(Dictionary)
A **symbol table** or **dictionary** is an abstract structure that associates a ***value*** with a ***key***.We use the **key** to search a data structure for the **value**(The key and valuewill be separate entities).Note for a given application we may need only the keys or only the values or both. We will define our symbol table / dictionary as an *interface*(with the idea that the dictionary specification does not require any specific implementation), and so there will be many different ways to implement a dictionary.
```Java
import java.util.Iterator;
public interface DictionaryInterface<K, V>
{
	public V add(K key, V value);
   
	public V remove(K key);
   
	public V getValue(K key);
   
	public boolean contains(K key);

	public Iterator<K> getKeyIterator();
   
	public Iterator<V> getValueIterator();
   
	public boolean isEmpty();
   
	public int getSize();
   
   	public void clear();
} // end DictionaryInterface
```
([DictionaryInterface.java](handout/DictionaryInterface.java)) — Note that standard Java has a similar interface called `Map(K,V)` — See API.

We could implement this interface using what we already know(underlying sorted array, or underlying linked list). Both of these implementations are similar in that the basic search involves **direct comparisons of keys**. In other words, to find a target key, `K`, we must **compare `K`** to one or more keys that are present in the data structure. However, if we change our basic approach perhaps we can get an improvement.

## Hashing
So let's try a different approach. Assume that:
1. We have an array (table), `T` of size **`M`**. 
2. We have a function `h(x)` that maps from our **key space** into indexes `{0,1,…,M-1}`. 
3. `h(x)` can be done in time proportional to the length of the key
Now how can we do an insert and find of some key x?
**Insert**
```
i = h(x);
T[i] = x;
```
**Find**
```
i = h(x);
if (T[i] == x) 
    return true;
else 
    return false;
```
This is the simplistic idea of hashing.

### Collision
However, Simple hashing fails in the case of a **collision**: **`h(x1) == h(x2)`, where `x1 != x2`**. I.e., when two distinct keys hash to the same location. Can we avoid collisions (i.e. guarantee that they do not occur)? Yes, but only when size of the **key space**, **$K$**, is **less than or equal to the table size**, **`M`**. When $|K| \leq M$ there is a technique called **perfect hashing** that can ensure no collisions. It also works if for$N \leq M$, but the keys are known in advance, which in effect reduces the key space to $N$.
- E.g.: Hashing the keywords of a programming language during compilation of a program

When $|K| > M$, by the ***pigeonhole principle***, collisions cannot be eliminated. We have more *pigeons* (potential keys) than we have *pigeonholes* (table locations), so at least 2 pigeons must share a pigeonhole. Unfortunately, this is usually the case

For example, an employer is using SSNs as the key:
	Let $M = 1000$ and $N = 500$.
	It seems like we should be able to avoid collisions, since our table will not be full. However, $|K| = 10^9$ since we do not know what the 500 keys will be in advance (employees are hired and fired, so in fact the keys change).

#### Reducing the number of collisions (Good Hashing)
Before discussing resolution, is there a way for us to keep the number of collisions in check? Yes, with a **good hash function**. The goal is to make collisions a (pseudo) "random" occurrence. Thus, collisions will occur, but due to chance, not due to similarities or patterns in the keys. So what is a good hash function?
1. It should **utilize the entire key** (if possible) and **exploit any differences between keys**
2. It should also utilize the **full address space** of the hash table

E.g., Consider hash function for on-campus Pitt students based on phone numbers, where $M = 1000$.
1. Attempt 1: First 3 digits of number
	$H(\verb|412-XXX-XXXX|) = \verb|412|$
	This is bad because the first 3 digits are area code and most people in this area live within a few area codes { 412, 724, etc }.
How can we do better? We can take phone number as an integer $\%\ M$. This is better because we are still getting 3 digits, but the last 3 digits don't have any special designation and tend to be pseudo-random.

E.g., Consider hash function for words into a table of size $M=1000$.
1. Attempt 1: Add ASCII values
	Ex: $H(\verb|"STOP"|) \implies 83 + 84 + 79 + 80 = 326$
	Problem 1: Does not fully exploit differences in the keys
		Ex: $H(\verb|"STOP"|) = H(\verb|"POTS"|) = H(\verb|"POST"|) = H(\verb|"SPOT"|)$
		Even though we use the entire key, we don't take into account the positions of the characters.
	Problem 2: Does not use the full address space
		Even small words will have $H(X)$ values in the 100s
		Even larger words will have $H(X)$ values well below 1000
		Thus for $M = 1000$ there will likely be collisions in the middle of the table and many empty locations at the beginning and the end of the table.
How can we do better? We can utilize all of the characters and the positions and all of the table. Consider integers and how they differ from each other($1234 \neq 4321 \neq 2341 \neq 3412$ … etc). They are diiferent because each digit has a different power of 10.$$1234 = 1*10^3+2*10^2+3*10^1+4*10^0$$$$4321 = 4*10^3+3*10^2+2*10^1+1*10^0$$We can do something similar for hash values of arbitrary strings. Using the same idea as above, we can apply the same idea to ASCII characters. Instead of 10, we have 256 ASCII characters, so let's multiply each digit by a different power of 256.
	Ex: $H(\verb|"STOP"|) = 83*256^3+84*256^2+79*256^1+80*256^0$
	Ex: $H(\verb|"POTS"|) = 80*256^3+79*256^2+84*256^1+83*256^0$
This will definitely distinguish the hash values of **all strings**. I.e., This will utilize all of the characters and positions. But what about utilizing all of the table? Recall that our table is size $M$ and observe that that these values will get very large very quickly. So, we can take the raw value $\%\ M$. This will likely "wrap" around the table many times, and should utilize all of the locations.

But how do we do this *in practice*? Notice how big the numbers will get – very quickly larger than even a `long` can store. If we use an `int` or even `long` the values will wrap and thus **no longer be unique** for each `String`. This is fine as it will just result in a collision. Also, calculating the values needs to be done in an efficient way so that $H(X)$ can be done quickly. There is an approach called ***Horner's method*** that can be applied to calculate the $H(X)$ values efficiently ([hashCode.java](handout/hashCode.java)).

One good approach to hashing is to:
1. Choose $M$ to be a prime number
2. Calculate our hash function as $h(x) = f(x)\mod M$ where $f(x)$ is some function that converts $x$ into a large "random" integer in an intelligent way. It is not actually random, but the idea is that if keys are converted into very large integers (much bigger than the number of actual keys) collisions will occur because of the pigeonhole principle, but they will be less frequent.

There are other good approaches as well

### Resolving Collisions
So we must redesign our hashing operations to work despite collisions. We call this **collision resolution**. There are two common approaches:

#### Open addressing
If a collision occurs at index `i` in the table, we try alternative index values until the collision is resolved. Thus a key may not necessarily end up in the location that its hash function indicates and we must choose alternative locations in a consistent, predictable way so that items can be located correctly. Our table can store **at most `M` keys**.

##### Linear Probing
The simplest open addressing scheme is **Linear Probing**. The idea is that if a collision occurs at location `i`, try (in sequence) locations `i+1`,` i+2`, … (**mod M**) until the collision is resolved.
For Insert: Collision is resolved when an empty location is found
For Find:
- Collision is resolved (**found**) when the **item is found**
Collision is resolved (**not found**) when an **empty location is found**, or when **index circles back to `i`**

###### Performance: $O(1)$ for Insert, Search for normal use
###### Issues
Consider a hash table of size $M$, that is *empty*, which uses a goof hash function. Given a random key, `x`, the probability that `x` will be inserted into any location `i` in the table is $\frac{1}{M}$. However, consider a hash table of size $M$ that has a **cluster of $C$** **consecutive locations** that are filled. Now, given a random key, `x`, the probability that x will be inserted into the location immediately following the cluster is $\frac{(C+1)}{M}$. Note that the probability of mapping `x` to a given location is still $\frac{1}{M}$, however for any `i` in the cluster $C$, `x` will end up after the cluster. Thus we have $C$ locations in the cluster plus 1 directly after it.

![[linear-probing-cluster.png]]
Recall how collisions in LP are resolved. Collision is resolved **found** when key is **found** (i.e., when the value is somewhere within the cluster), and collision is resolved **not found** when empty location is found, which requires us to traverse the entire cluster. Clearly as the clusters get longer we need more probes in both situations, but especially for not found. As the ***load factor***, $\alpha = \frac{N}{M}$, increases (i.e., as the table fills with keys), cluster sizes begin to approach $M$, and search times will now degrade from $\theta{(1)}$ to $\theta(M) = \theta(N)$
- Note here that $\theta(M) = \theta(N)$ in this case since our table can hold at most M keys.
EOL.23

---

How can we "fix" (or at least improve) this problem? Even after a collision, we need to make all of the locations available to a key. This way, the probability from filled locations will be redistributed throughout the remaining empty locations in the table, rather than just being pushed down to the first empty location after the cluster. Ideally, if we have $C$ locations filled and ($M-C$) locations empty, we make it so that:
1. P(insert at a filled location) == 0
2. P(insert at any empty location) == 1/M + (C/M)/(M-C) — There is a probability of C/M that the hash value will be to a filled location.
Instead of the insert probability of the C locations falling on just a few locations. 

I.e., we'd like that probability to be divided evenly amongst the ($M-C$) remaining open locations. Now after a collision at index `i`, a key would still be equally likely to be inserted at any remaining open location. We won't achieve this ideal but we can come close.
Attempt 1. How about making the increment 5 instead of 1?
	This won't work. We are still making clusters, they are just physically separated from each other. Any constant increment will have the same result.

##### Double Hashing
What if we increment the index by an increment that is determined by a different hash function, $h_2(x)$, instead of a constant? This way keys that hash to the same location will likely not have the same increment. $h_1(x_1) == h_1(x_2)$ with $x_1 != x_2$ is bad luck (assuming a good hash function). However, ALSO having $h_2(x_1) == h_2(x_2)$ is REALLY bad luck, and should occur even less frequently. It also allows for a collided key to move (mostly – depending on $h_2(x)$) anywhere in the table

Note that we still get collisions with Double Hashing, and we can even get multiple collisions in one operation. In this case we iterate just as we do with Linear Probing, using the Double Hashing increment multiple times. However, because $h_2(x)$ varies for different keys, it allows us to spread the data throughout the table, even after an initial collision. But we must be careful to ensure that double hashing always "works"
	- Make sure increment is > 0
	- E.g., note the +1 in our $h_2(x): h_2(x) := (x\mod 7) + 1$
Our mod operator can result in 0, which is fine for an absolute address, but not for an increment.

Also, we want to make sure no index is tried twice before all are tried once
E.g., Consider:
|Index|Value|
|:---:|-------|
|0||
|1|V|
|2||
|3|W|
|4||
|5|X|
|6||
|7|Y
and $h(\verb|Z|) = 3$, $h_2(\verb|Z|) = 2$. If we search the table, we would never find an open spot. In order to fix this, we can make $M$ a prime number. 

Note that these were not issues for linear probing, since the increment is clearly > 0 and if our increment is 1 we will clearly try all indices once before trying any twice.

As $\alpha$ increases, double hashing shows a definite improvement over linear probing.
However, as $\alpha \to 1$ (or as $N \to M$), both schemes degrade to $\theta{(N)}$ performance. Since there are only $M$ locations in the table, as it fills there become fewer empty locations remaining, and multiple collisions will occur even with double hashing. This is especially true for inserts and unsuccessful finds. Both of these continue until an empty location is found, and few of these exist. Thus it could take close to $M$ probes before the collision is resolved. Since the table is almost full $\theta(M)$ 

##### Issues with Open Addressing
We have seen that performance degrades as $N$ approaches $M$. Typically for open addressing we want to keep the table partially empty.
	- For linear probing, $\alpha=\frac{1}{2}$ is a good rule of thumb
	- For double hashing, we can go a bit higher (3/4 or more)
How can we do this?
Monitor the logical size (number of entries) vs. physical size (array length) to calculate $\alpha$, and resize the array and rehash all of the values when $\alpha$ gets past the threshold. However, rehashing all of the data seems like a LOT of work.

What about delete?
Suppose the following Linear Probing table:
|Index|Value|
|:---:|-------|
|0||
|1|W|
|2|X|
|3|Y|
|4|Z|
|5||
|6||
|7||
 and assume $H(\verb|Z|) = 2$ but it was placed in index `4` due to a collision. Search for Z would try `2`, `3`, `4`, finding `Z` at location `4`. Now delete(`Y`) and search for `Z` again. The search would now stop at index `3` with **not found** even though **`Z` is present**. Deleting `Y` broke the chain. How can we fix this? 

One solution is to rehash all keys from deleted key to end of cluster. In this case `Z` still hashes to `2` and will move to position `3` and once again be within the chain. But this will require a lot of work, especially if the item at the front of the chain is removed. Also it will not work with double hashing.

For double hashing, we could consider 3 states for every location in the table:
1. **Empty:** 
	Will stop an insert (put value there)
	Will stop a find (return not found)
2. **Full:**
	Will NOT stop an insert (keep looking)
	Will NOT stop a find (inside a cluster)
3. **Deleted:**
	Will stop an insert (re-use the location)
	Will NOT stop a find (WAS inside a cluster and we don't want to stop the search)

The text also uses this approach for linear probing. Although this will work, over time using the table few "Empty" spots will remain.

#### Closed addressing
Each index `i` in the table represents a collection of keys. Thus a collision at location `i` simply means that more than one key will be in or searched for within the collection at that location. The number of keys that can be stored in the table depends upon the maximum size allowed for the collections

##### Separate Chaining
The most common form is **separate chaining**, where we use a simple linked-list at each location in the table.

###### Performance
Clearly a **not found** search must traverse entire chain, thus performance is dependent upon chain length. Note that chain length is determined by the load factor, $\alpha$. Note that the average chain length = (total # of nodes)/($M$), and that (total # of nodes) = $N$, thus average chain length = $\alpha$.

As long as $\alpha$ is a small constant, performance is still $\theta(1)$. Also, $N$ can now be greater than $M$. Also this has more graceful degradation than open addressing schemes. However, if $N >> M$, then it can still degrade to $\theta(N)$ performance. Thus we may still need to resize the array when a gets too big. Note that a *poor hash function* can also degrade this into $\theta(N)$.

### Implementing a Hash Table
So far we have discussed hashing primarily in an abstract way. But how would we actually implement this in Java? Recall our `DictionaryInterface<K,V>` and have a hash table that implements this interface.
`public class HashedDictionary<K, V> implements DictionaryInterface<K, V>`

First, define the `Entry<K, V>` Inner Class, which will be the basis for our key, value pairs. Our hash table will be a collection of Entry<K,V> objects.
```Java
protected final class Entry<K, V>
{
	private K key;
	private V value;    
	private Entry(K searchKey, V dataValue)
	{
             key = searchKey;
             value = dataValue;
	} // end constructor		
	private K getKey()
	{
	     return key;
	} // end getKey		
	private V getValue()
	{
	     return value;
	} // end getValue		
	private void setValue(V newValue)
	{
	     value = newValue;
	} // end setValue
} // end Entry
```
Next, we need an array of `Entry<K,V>` for our table:
```Java
private Entry<K, V>[] hashTable;
private int tableSize; 
private int numberOfEntries;
```
And a constructor:
```Java
public HashedDictionary(int initialCapacity)
{
	initialCapacity = checkCapacity(initialCapacity);
	numberOfEntries = 0;
	int tableSize = getNextPrime(initialCapacity);
	checkSize(tableSize); 		
	Entry<K, V>[] temp = (Entry<K, V>[])new Entry[tableSize];
	hashTable = temp;
	initialized = true;
} // end constructor
```

We, now add the methods:
```Java
public V add(K key, V value)
{
      if ((key == null) || (value == null))
         throw new IllegalArgumentException("Cannot add null to a dictionary.");
      else
      {
	V oldValue;
	int index = getHashIndex(key);

	if ( (hashTable[index] == null) || (hashTable[index] == AVAILABLE) )
	{ // Key not found, so insert new entry
	     hashTable[index] = new Entry<>(key, value);
	     numberOfEntries++;
	     oldValue = null;
	}
	else
	{ // Key found; get old value for return and then replace it
	     oldValue = hashTable[index].getValue();
	     hashTable[index].setValue(value);
	} // end if
         
	if (isHashTableTooFull())
	     enlargeHashTable();
         return oldValue;
      } // end if
   } // end add
```
```Java
private void enlargeHashTable()
{
      Entry<K, V>[] oldTable = hashTable;
      int oldSize = hashTable.length;

      int newSize = getNextPrime(oldSize + oldSize);
      checkSize(newSize); // Check that the prime size is not too large
      Entry<K, V>[] tempTable = (Entry<K, V>[])new Entry[newSize]; 
      
      hashTable = tempTable;
      numberOfEntries = 0; // Reset number of dictionary entries, since
                           // it will be incremented by add during rehash

      // Rehash dictionary entries from old array to the new and bigger array;
      // skip both null locations and removed entries
      for (int index = 0; index < oldSize; index++)
      {
         if ( (oldTable[index] != null) && (oldTable[index] != AVAILABLE) )
            add(oldTable[index].getKey(), oldTable[index].getValue());
      } // end for
} // end enlargeHashTable
```
See: [HashedDictionary.java](handout/HashedDictionary.java) for `getValue(k)`, `remove(k)`, and `iterator`s

Also note that in a separate chaining hash table, we would still have an array, but instead of an array of Entry<K,V> it is an array of linked lists of Entry<K,V>, and each location represents the front of a linked list.

eol.24

---

