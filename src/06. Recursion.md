# Recursion
## Introduction
The idea behind recursion (and a hint for when to use recursion) is that some problem $P$ is defined/solved in terms of one or more problems $P'$, which are **identical in nature** to $P$ but _smaller in size_. To use recursion, the problem must have:
- One or more **base cases** in which no recursive call is made
- One or more **recursive cases** in which the algorithm is defined in terms of itself
- The recursive cases must eventually lead to a base case

## Simple Examples
The examples in this section are designed to help you think about how to apply recursion to a problem. These simple examples are not good examples of _when_ to use recursion, just _how_ to use recursion to solve a problem.

### Factorial ($N!$)
A lot of recursive problems have their origins in mathematics. The factorial of a _positive integer_ $N$ is defined as the **product of all of the positive integers between $1$ and $N$** (inclusive). In other words: $N! = N * (N-1) * (N-2) * ... * 1$ Additionally, $0!$ is often defined as $1$. Let's look at how to write a recursive implementation:
- $N! = N * (N-1)!,\ (when\ N > 0)$
- $N! = 1,\ (when\ N = 0)$

Let's look at our 3 requirements. The Factorial has:
- At least one base case:
	- $N! = 1,\ when\ N = 0$
- At least one recursive case:
	- $N! = N * (N-1)!,\ when\ N > 0$
- Recursive case eventually leads to base case
	- Since the recursive case has argument of N-1, it should always lead to a base case...but does it always?

### Integer Powers ($X^N$)
A number ($X$) raised to a positive integer ($N$) is defined as multiplying $X$ by itself $(N-1)$ times. For example, $X^2 = X \cdot X$.
What is a recursive definition of $X^N$? What would be our 3 requirements:
- At least one base case
	- $X^N=1,\ when\ N=0$
- At least one recursive case
	- $X^N=X \cdot x^{(N-1)},\ when\ N>0$
* Recursive case eventually leads to base case
	- Decrementing of $N$ gives similar situation to that of factorial. Normally base case is always reached, unless $N$ is initially negative

## Writing Recursive Methods
Many recursive methods are very similar to the underlying definitions. Let's look at Factorial:

```Java
public long factorial (int N)
{
   if (N < 0)
       throw new IllegalArgumentException();
   if (N <= 1)  // Isn't N = 0 the base case?  discuss
       return 1;
   return N * factorial(N-1);
}
```

Note that negative $N$ generates an exception. Notice also that the method is calling itself and uses the result in the return expression.

## How does recursion work?
There are two important ideas that allow recursion to work: **Activation Record** and **Run-Time Stack**.

An **Activation Record** is a block of memory allocated to store arguments for method parameters, local variables, and the return address during a method call. An activation record is assocated with each method _call_, so if a method is called multiple times, multiple records are created. For example, with the factorial method above and this method call:
```Java
long result = factorial(3);
```
The activation record might look like this:
![[Factorial-activation-record.png]]
The return address is the set by the operating system. It refers to the next instruction to execute once the method call is finished. In this particular example, the next instruction would probably be something like "save the returned value into the variable `result`".

The **Run-Time Stack** is an area of the computer's memory which maintains activation records in Last In First Out (LIFO) order. Recall our previous discussion of Stacks.

When a method is called, an activation record containing the parameters, return address, and local variables is **pushed onto the top of the run-time stack**. If the method subsequently **calls itself, a new, distinct activation record** containing new data is pushed onto the top of the run-time stack. The **activation record at the _top_ of the run-time stack** represents the **currently-executing call**. Activation records below the top represent previous calls that are waiting to be returned to. When the **top call terminates**, control returns to the address from the top activation record and then the **top activation record is popped from the stack**.

See [Example 10.java](handout/Example10.java)

## An Example: Recursive Sequential Search
Let's look at one more simple example: Sequential Search, where we find a key in an array or linked list by checking each item in sequence. We know how to do this iteratively, it's basically a simple loop to go through each item. Let's now look at how to do it recursively. Remember, we always need to consider the problem in terms of a smaller problem of the same type.

In order to search for a key in an array of length $N$ we check the length:
If `length == 0`, we are done (base case not found)
How is "length" changed between calls? Logical length is checked and updated via indices.
Note. This is commonly done with recursive methods on arrays
We don't change the physical array size – why?
In recursive algorithms on arrays, we will usually update the "length" of the array logically by changing beginning and/or ending index values. To actually reduce the size of the physical array would require allocating a new, smaller array which would add an incredible amount of overhead to the recursive process.
Else check the first element of the array
If first `element == key`, we are done (base case found)
Else Sequential Search the remaining $N-1$ elements (recursive case)
Once we have this idea, we can quickly convert it into code:

```Java
public static <T extends Comparable<? super T>>
		int recSeqSearch(T [] a, T key, int first)
{
	if (first >= a.length)
		return -1; //No more data so return -1 to indicate not found
	else if (a[first].compareTo(key) == 0)
		return first; //Data is found and return index
	else return recSeqSearch(a, key, first+1); //Logical size is reduced by one
}
```

What about a `LinkedList` implementation?
Note that we must consider the special-base case of `list.getNextNode() == null`

```Java
public static <T extends Comparable<? super T>>
		int recLinkedSearch(Node<T> list, T key, int loc)
{
	if (list == null)
		return -1; // LogicalSize =0; Return -1 to indicate not found
	else if (list.getData().compareTo(key) == 0)
		return loc; //First item matches key; Data found, Return index
	else
		return (recLinkedSearch(list.getNextNode(), key, loc+1));
		// Recursively search the list moving down one node, LogicalSize --;
}
```

Note that we are still keeping an index even though we don't use it to access the list. This is for the return value which must be a position in the list.

## Divide and Conquer

So far, the recursive algorithms that we have seen (see text for more) are simple, and probably would NOT be done recursively. The iterative solutions work fine and are probably more intuitive and easier to implement. They were just used to demonstrate _how_ recursion works. However, recursion often suggests approaches to problem solving that are more logical and easier than without it. For example, **divide and conquer**.

The idea behind **divide and conquer** algorithms is that a problem can be solved by breaking it down to one or more "smaller" problems in a systematic way. The subproblem(s) are usually a **fraction of the size** of the original problem and are usually **identical in nature** to the original problem. By doing this, the overall solution is just the result of the smaller subproblems being combined. If this sounds similar to recursion, that's because it's often implemented recursively. The key difference is that divide and conquer makes more than one recursive call at each level (unlike just one in the examples above).

We can think of each lower level as solving the same problem as the level above. The only difference in each level is the size of the problem, which is 1/2 (sometimes some other fraction) of that of the level above it. Note how quickly the problem size is reduced.
![[divide-conquer-graph.png]]

### Example: Integer Exponentiation

Let's look at one of our earlier recursive problems: Power function ($X^N$). We have already seen a simple iterative solution using a for loop. We have also already seen and discussed a simple recursive solution. Note that the recursive solution does recursive calls rather than loop iterations. However both algorithms have the same runtime: we must do $O(N)$ multiplications to complete the problem. Can we come up with a solution that is better in terms of runtime? Let's try Divide and Conquer. We typically need to consider two important things:

1. How do we break up or "divide" the problem into subproblems?
   - In other words, what do we do to the data to process it before making our recursive call(s)?
1. How do we use the solutions of the subproblems to generate the solution of the original problem?
   - In other words, after the recursive calls complete, what do we do with the results?
   - You can also think of this is "how do we put the pieces back together?"

For $X^N$, the problem "size" is the exponent $N$. So, a subproblem would be the same problem with a smaller $N$. Let's try cutting $N$ in half so each subproblem is of size $\frac{N}{2}$. This means defining $X^N$ in terms of $X^{\frac{N}{2}}$ (don't forget about the base case(s)). So, how is the original problem solved in terms of $X^{\frac{N}{2}}$?
$$X^N=(X^{\frac{N}{2}})(X^{\frac{N}{2}})=(X^{\frac{N}{2}})^2$$
Can we still use the same base case: $X^0=1$? Let's consider the case $2^3$.$$2^3=(2^1)^2=((2^0)^2)^2=(1^2)^2=1\neq8=2^3$$What went wrong? Well the solution above works only when the exponent is _EVEN_.  If the exponent is odd, due to integer division truncation, we lose information.  We need a different case here.  Let's try again.

- $X^N=(X^{\frac{N}{2}})^2$  when $N$ is even and > 0.
- $X^N=X(X^{\frac{N}{2}})^2$ when $N$ is odd and >0.
- $X^N=1$ when $N=0$

The divide and conquer approach could look something like this, where solving XN means computing XN/2 twice (using two recursive calls), then multiplying the results together once the recursive calls return.

So, will this be an improvement over the other 2 versions of the function?

- Problem size is being cut in half each time
- Informal analysis shows we only need $O(\log_2N)$ multiplications in this case (see text)
  This is a big improvement over $O(N)$.

```Java
public static final BigInteger two = new BigInteger("2");
```

```Java
public BigInteger Pow3(BigInteger X, BigInteger N)
{
    if (N.compareTo(BigInteger.ZERO) == 0)		// N == 0
        return new BigInteger("1");		// return 1
    else
    {
        BigInteger NN = N.divide(two);    // Get N/2
        BigInteger temp = Pow3(X, NN);    // Make recursive call
        if ((N.mod(two)).compareTo(BigInteger.ZERO) == 0)  // N is even
        {
             return (temp.multiply(temp));     // temp * temp
        }
        else                                   // N is odd
        {
             BigInteger MM = temp.multiply(temp);
             return (MM.multiply(X));           // X * temp * temp
        }
    }
}
```

Note that we are using BigInteger, so syntax uses method calls.

### Pitfalls of Implementing Divide & Conquer

Always remember that recursive calls spawn other recursive calls. It is very important to understand how these calls proceed when analyzing / utilizing recursive algorithms. Sometimes a correct algorithm is not necessarily a desirable one, even if it “seems” good

Consider $X^N$ as we have just discussed. Let’s look at the problem expressed in two different, but mathematically equivalent ways.

1. Possible Implementation 1

- $X^N = (X^{\frac{N}{2}})^2$, $N$ even, >0
- $X^N = X*(X^{\frac{N}{2}})^2$ $N$ odd, > 0
- $X^N=1$, $N=0$
- ![[dc-bad-example-tree.png]]

2. Possible Implementation 2

- $X^N = (X^{\frac{N}{2}})(X^{\frac{N}{2}})$, $N$ even, >0
- $X^N = X*(X^{\frac{N}{2}})(X^{\frac{N}{2}})$ $N$ odd, > 0
- $X^N=1$, $N=0$
- ![[dc-good-example-tree.png]]

Notice that the analysis for this is similar to that of binary search. Run-time stack grows to height of approx. $\log_2{N}$, and at each level we do either one or two multiplications (1. One for the square—always, 2. One for odd N—sometimes)

Also notice that both implementations are mathematically identical, however, from a programming point of view, they are not. Implementation (2) is _terrible_, because each call spawns two additional calls, and thus does $O(N)$ multiplication, which is same as non-divide and conquer.

Lastly, in our analysis, we calculated the “number of multiplication”, assuming (more or less) that a multiplication can be done in $O(1)$. However, this is not true, and in fact, it is a function of the number of bits in $X$(See CS 1501). If $X$ is an `int` or `long` we can think of this time as constant, as the number of bits is fixed. If $X$ is arbitrarily large then the run-time for the multiplication will also grow quickly. Also, even storing / allocating space for the numbers will be time consuming (think: a square “doubles” the bits). For very large numbers a powerMod() method may be more useful:  $X^N$ mod $Z$ for some $Z$. This will restrict the maximum size of the result and will be discussed in CS 1501.

### Example: Binary Search

Now let's reconsider **binary search**, this time using using recursion with divide and conquer. Recall that the data must be **in sorted order** already and we are searching for some object S.

- How do we divide?
  - Cut the array in half, which is what the iterative version does
- How do we use the subproblem results to solve the original problem?
  - We may not really need to do anything here at all.
- What about base case? There are actually two:
  - Array size is down to zero (S not found)
  - Key matches current item (S found)
- What about the recursive case? Consider the middle element, M, and check if S is: - Equal to M: you are done and you have found it (base case) - Less than M: recurse to the left side of the array - Greater than M: recurse to the right side of the array
  This is the same logic as the iterative case. Proceeding in this fashion removes 1⁄2 of the remaining items from consideration with each guess.

All we are doing when a call returns is to "pass on the result". Also, we return BinarySearch of left or right side. So second part of our D and C approach here is trivial Just like the iterative version, we cut the problem in half.

- In iterative it is cut in half in each iteration
- In recursive it is cut in half with each recursive call
  Let's compare this to iterative binary search (Ref. Chapter 19 of Text)
- Counts the comparisons required for the searches
- Clearly as $N$ gets larger the difference becomes quite significant

```Java
public <T extends Comparable<? super T>>
       int binarySearchr (T [] a, T obj, int low, int high)
{
     int ans;
     if (low <= high) //low>high => basecase not found. skip & return -1
     {
	 int mid = (low + high)/2;
	 T midItem = a[mid];
	 int res = midItem.compareTo(obj); //compare middle item to key
	 if (res < 0) //middle item is < key – recurse to right side
	      return (binarySearchr(a, obj, mid + 1, high));
	 else if (res > 0) //middle item is > key – recurse to left side
	      return (binarySearchr(a, obj, low, mid - 1));
	 else
	      return mid; //middle item is == key – base case found
     }
     return -1;
}

```

## Tail Recursion and Recursion Overhead

So far, every recursive algorithm we have seen can be done easily in an iterative way. Even the divide and conquer algorithms (Binary Search, Integer Exponentiation) have simple iterative solutions. Can we tell if a recursive algorithm can be easily done in an iterative way? Yes, any recursive algorithm that is exclusively tail recursive can be done simply using iteration without recursion. Most algorithms we have seen so far are exclusively tail recursive.

**Tail recursion** is a recursive algorithm in which the recursive call is the **_last_** statement in a call of the method. If you look back at the algorithms we've seen so far, this is generally true (ignore trace versions, which add extra statements). The integer exponentiation function does some math after the call, but it can still be done easily in an iterative way, even the divide and conquer version. In fact, any tail recursive algorithm can be converted into an iterative algorithm in a methodical way.

- In fact, some compilers do this automatically

Why bother with identifying tail recursion and know that it's possible to convert to an iterative method? Recursive algorithms have overhead associated with them:

- Space: each activation record takes up memory in the run-time stack
  - If too many calls "stack up" memory can be a problem
  - We saw this when we had to increase the stack size for [BSTest.java](handout/BSTest.java)
- Time: generating activation records and manipulating the run-time stack takes time
  - A recursive algorithm will always run more slowly than an equivalent iterative version

These are implementation details that algorithm analysis ignores. An iterative implementation of binary search and a recursive implementation will have the same runtime according to algorithm analysis. Recall that algorithms are independent of programming language, hardware, and implementation details. Deciding which algorithm to use and how to implement it are two separate, but very important, questions.

If recursive algorithms have this overhead, why bother with it? For some problems, a recursive approach is more natural and simpler to understand than an iterative approach. Once the algorithm is developed, if it is tail recursive, we can always convert it into a faster iterative version (ex: binary search, integer exponentiation). However, for some problems, it is very difficult to even conceive an iterative approach, especially if multiple recursive calls are required in the recursive solution. We'll take a look at some examples in the next section.

## Backtracking

The idea behind backtracking is to proceed forward to a solution until it becomes apparent that no solution can be achieved along the current path. At that point, **_undo_** the solution (backtrack) to a point where we can proceed forward again and look for a solution.

### Example: Traversing a maze

At an intersection we have a choice:
We choose a path, and if it leads to a dead end, we need to go back to that intersection and try a different path. We may need to do this multiple times before getting out of the maze.

So our goal is to move forward toward the solution. Each move forward will be a recursive call. At some / many moves we will make a choice about how to proceed. If we get stuck on a path – we know that we cannot reach a solution from there, we backtrack to a previous call. At that call we then change our choice and try to move forward again. Eventually we find a solution or have tried all possible choices and determined that a solution does not exist.

(End of Lecture 13)

---

### Example: 8 Queens Problem

In the 8 Queens Problem, you attempt to find an arrangement of queens on an `8x8` chessboard such that no queen can take any other in the next move. In chess, queens can move horizontally, vertically, or diagonally for multiple spaces.

How can we solve this with recursion and backtracking? All queens must be in different rows and different columns, so each row and each column must have exactly one queen when we are finished. Complicating it a bit is the fact that queens can move diagonally. So, thinking recursively, we see that to place 8 queens on the board we need to:

1. Place a queen in a legal (row, column)
2. Recursively place 7 queens on the rest of the board

Where does backtracking come in? Our initial choices may not lead to a solution; we need a way to undo a choice and try another one.

The basic idea of the method is that **each recursive call** attempts to place a queen in a **specific column**, `col`. Within a call, a loop is used, since there are 8 rows in the column. For a given method call, **the state of the board from previous placements is known** (i.e. where are the other queens?). This is used to determine if a square is legal or not. If a placement within the column does not lead to a solution, the queen is removed and moved down one row in that column. When all rows in a column have been tried, the call terminates and **backtracks to the previous call** (in the previous column). If a queen cannot be placed into column `i`, do not even try to place one onto column `i+1`; rather, backtrack to column `i-1` and move the queen that had been placed there.

See [JRQueens.java](handout/JRQueens.java)

This solution is fairly elegant when done recursively. To solve it iteratively, it's rather difficult. We need to store a lot of state information (such as for each column so far, where has a queen been placed?) as we try (and un-try) many locations on the board. The run-time stack does this automatically for us via activation records. Without recursion, we would need to store / update this information ourselves. This can be done (using our own Stack rather than the run-time stack), but since the mechanism is already built into recursive programming, why not utilize it?

### Example: Finding Words in a 2-D Grid of Letters

We are given a grid and a word located somewhere within the grid. Each letter must touch the previous letter, and we can move right / down / left / up. Also, we can only use a letter once in a word.

How would we solve this problem recursively? Each recursive call will consider one location on the board. As recursive calls "stack up" we match letters in our board with the characters in the word. If we match all of the characters in a word then we are done. If a word is K letters long we would have K calls built up on the run-time stack. The last call would match the last letter in the word. If we get stuck within a word we will backtrack to try a different direction in our match.

`public boolean findWord(int r, int c, String word, int loc, char [][] bo)`

- `r` and `c` are the current row and column
- `word` is the word we are trying to match
- `loc` is our current position in the word
- `bo` is our character grid
  Given a call at position r, c on the board, our recursive calls would be:
- `findWord(r, c+1, word, loc+1, bo) // right`
- `findWord(r-1, c, word, loc+1, bo) // up`
- `findWord(r, c-1, word, loc+1, bo) // left`
- `findWord(r+1, c, word, loc+1, bo) // down`
  Note, we would only try the next one if the previous one failed.
  See [FindWord.java](handout/FindWord.java)

### Example: Tower of Hanoi Problem

This is a simple puzzle and popular recursive algorithm problem. There are three rods. Two are empty and the third holds _N_ disks. Each disk has a different diameter and they are stacked from widest to narrowest.

![Tower of Hanoi Setup](https://upload.wikimedia.org/wikipedia/commons/0/07/Tower_of_Hanoi.jpeg)

The goal of the puzzle is to move the entire stack to another rod, obeying the following rules:

- Move only one disk at a time
- You can only move the top disk on a stack
- Do not place a wider disk on top of a narrower disk

See [JRHanoi.java](handout/JRHanoi.java).

How can we solve this recursively?
We have $N$ disks and 3 towers (call them `start`, `mid`, `end`)
To move $N$ disks from `start` to `end`

1. We can move $N-1$ disks from `start` to `mid`
2. We can then move the last disk from `start` to `end`
3. We can then move $N-1$ disks from `mid` to `end`

But wait, can't we only move one disk at a time? Yes, but the solution is recursive, it is simply solving the Hanoi problem with $N-1$ disks. The actual move of a disk is always done only 1 at a time.

```Java
public void solveHanoi(int sz, int strt, int mid, int end)
	{
	if (sz == 1)
		System.out.println("Move from " + strt + " to " + end);
	else
	{
		solveHanoi(sz-1, strt, end, mid);
		System.out.println("Move from " + strt + " to " + end);
		solveHanoi(sz-1, mid, strt, end);
	}
}
```

When a recursive algorithm has 2 calls, the execution trace is now a binary tree, as we saw with the solution to Tower of Hanoi. This execution is more difficult to do without recursion. However, it is possible. To do it, programmer must create and maintain their own stack to keep all of the various data values. This increases the likelihood of errors / bugs in the code. Later, we will see some other classic recursive algorithms with multiple calls, such as MergeSort and QuickSort.

Let's analyze the asymptotic run-time for our Towers of Hanoi solution. Consider a "move" to be our key instruction.

1. The execution trace has exactly $N$ levels.
2. At each call of size $X$, we have to do $1$ move. - I.e., Every recursive call moves exactly one disk
   Thus, our number of moves is determined by our total number of calls. If our calls double at each level we get:$$1+2+4+...=2^0+2^1+2^2+...+2^{N-1}=2^N-1\implies{O(2^n)}$$ This is very poor—exponential.

(End of Lecture 16)

---
