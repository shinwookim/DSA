# Algorithm Analysis

## Introduction
So far, we have talked about differences in efficiency between various implementations of the ADTs, but we have been somewhat vague about it. Now we will look at algorithm efficiencies in a more formal, mathematical way. Why do we care about formalizing this? Consider all of the work involved in implementing a new ADT. It is non-trivial to get all of the operations working correctly. Plus, many special cases and much debugging is required during implementation. If we can determine the best implementation before implementing the ADT, it can save us a lot of time. Inefficient potential implementations could be abandoned before they are even started!

See Sum of integers example in Textbook Sections 4.1-4.2

E.g., Searching a sorted array
Assume the array contains $N$ items in sorted order. A sequential search will require up to $N$ tests to find the item. A binary search will take at most $\log_2{N}$ tests to find the item.

Consider tests for 1 search:
|Sequential Search($N$)|Binary Search($\log_2{N}$)|
|--|--------|
| 8 | 3|
|16|4|
|32|5|
|64|6|
|...|...|
|1024|10|
|1M|20|
|1G|30|

What if we were doing 1 million searches instead of one? Suppose we have to do 1 million searches on 1 million items:
$\verb|(# of searches)|$ \* $\verb|(# of tests per search)|$
- Sequential Search: $1M * 1M = 1T = 10^{12}$
- Binary Search: $1M * 20 = 20M = 2*10^{7}$

If each test takes one nanosecond ($10^{-9}$ seconds):
- Sequential Search: $10^{12}(10{-9})$ = $10^{3}$ seconds = 16.6666 minutes
- Binary Search: $(2 * 10^7) * 10^{-9}$ = 0.02 seconds

The difference is amazing. Just rethinking our algorithm takes us from something that would take hours to something that just takes a fraction of a second. Other examples can have even more extreme differences. CS 1501 will have many examples. By analyzing our algorithm **_before_** implementing it, we can thus avoid implementing algorithms that will require too much time to run. A little analysis saves us a lot of programming.

## Measuring Execution Time
How can you compare execution times of algorithms? Perhaps the most obvious approachis to time them **empirically**. This will give us actual run-times that we can use to compare. This is very useful for algorithms/ADTs that have already been implemented. However, we said previously that often it is good to get a ballpark on the runtime of an algorithm/ADT **before** actually implementing it. Perhaps we wouldn't want to go through the effort if the algorithm is not going to be useful. Additionally, measuring implementations depend on things independent of the ADTs/algorithms themselves, such as programming language, computer hardware, and input/data chosen.

The preferred approach is to not actually time a program, even if such a program exists. Instead, we use **asymptotic analysis**, which follows this procedure:
1.  Determine some **key instruction** or group of instructions that controls the overall run-time behavior of the algorithm
    - For example, with **sorting** we need to **compare** items to each other. Even though sorting involves other instructions, we can say that the overall run-time is **directly proportional** to the number of comparisons done.
2.  Determine a **formula / function** for how the number of key instructions increase as the problem size increases (typically we use the variable $N$ for the problem/input/data size)
3.  Determine the _case_ you want to deal with. We typically are concerned with two different cases:
    - **Worst Case Time**: What is the formula for the **maximum** number of key instructions relative to $N$? We should know what the worst case time can be so that we can plan for it if necessary.
    - **Average Case Time**: What is the formula for the **average** number of key instructions relative to N? How will the algorithm do normally?
4.  Only worry about the order of magnitude. We use the measure **Big-O** for this (although you will later learn about Big Omega, Big Theta, and the little versions of all three). Basically, with Big-O, we ignore lower order terms and constant multipliers.

E.g., let's say we determine the formula for the comparisons for a given sorting algorithm in the worst case to be $F(N) = (N^2/2) - (N/2)$ We say the Big-O run-time of this sorting algorithm is $O(N^2)$.
Notice that we ignore...
-  **lower order terms** because they become less significant as the problem size increases
- Think about $N^2$ vs $N^3$ when $N = 10$ and when $N = 100$ and when $N = 1000$
- This is why we call it **asymptotic** analysis
- **constant multipliers** because they can depend on programmer, programming language, computer, etc. 
	* Program A is written by Alice and runs in time 4N. Program B is written by Bob and runs in time 2N. Maybe Alice is a better programmer. Maybe Alice used a better compiler than Bob.
## Some Examples
Let's take a look at some examples:
1. Constant time $O(1)$ – independent of $N$
```
Y = X;
Z = X + Y;
```
2. Linear time $O(N)$ – grows at same rate as $N$
```
for (int i = 0; i < N; i++)
	do_some_constant_time_operation
```
3. Quadratic Time $O(N^2)$
```
for (int i = 0; i < N; i++)
	for (int j = 0; j < N; j++)
		do_some_constant_time_op;
```
There are many (infinitely) others, including some we will see soon.

## Run-time Analysis of Search Algorithms
Recall our previous discussion of *sequential search* versus *binary search*. What is the key instruction we want to measure? Key instruction is a **comparison**.

We will consider the worst case behavior.
```Java
public static int sequentialSearch(Object[] a, Object key) {
    int i = 0;
    while (i < 0) {
        if (((Comparable)a[i]).compareTo(key))
            return i;
        i++;
    }
    return -1;
}
```
For **sequential search**, the aymptotic runtime is $O(N)$ because there is a single while loop with up to $N$ iterations / comparison.

For **binary search**, the aymptotic runtime is $O(\log_2{N})$, but the runtime analysis is a bit trickier. It has a loop like sequential search, but now the number of iterations is very different. Let's look at the code from the standard Java library in the [java.util.Arrays](http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html) class:
```Java
public static int binarySearch(Object[] a, Object key) {
    int low = 0;
    int high = a.length-1;
    while (low <= high) {
        int mid =(low + high)/2;
        Object midVal = a[mid];
        int cmp = ((Comparable)midVal).compareTo(key);
        if (cmp < 0)
            low = mid + 1;
        else if (cmp > 0)
            high = mid - 1;
        else
            return mid; // key found
    }
    return -(low + 1); // key not found.
```

The worst case occurs when the data is not found (when `low` becomes greater than `high`)

Let's simplify things a bit. First, assume that in each iteration, the array is cut exactly in half. In reality, this won't quite be true, but it's close enough. Second, assume that the initial size of the array is exactly a power of two (i.e. $2^k$ for some positive integer $k$). While this will rarely be true, it makes the analysis easier (and has no effect on our results).

Let's combine these simplifying assumptions and apply them to determine the runtime. To determine the problem size at each iteration:

0.  At iteration 0, $N_0 = 2^k$
1.  At iteration 1, $N_1 = N_0 / 2 = 2^{k - 1}$
2.  At iteration 2, $N_2 = N_1 / 2 = N_0 / 2^2 = 2^{k - 2}$
3.  ...
4.  At iteration $k$, $N_k = N_0 / 2^k = 2^{k - k} = 1$

So, if we don't find the element in the array (i.e. we're in the worst case), then we have $k+1$ iterations. At each iteration, we do one comparison, so this yields $k+1$ comparisons. But we need this in terms of $N$. From our definition above:
$$N = 2k$$
$$\log_2{(N)} = k$$
This makes $k+1 = \log_2(N) + 1$, which makes our final answer $O(\log_2(N))$.
Note that our simplifying assumptions do not change our final answer. It might change the result by an iteration or two but the Big-O will still be the same, as constants are dropped.

## Run-times and Amortized Time Bag ADT
We've seen three implementations of the bag ADT. We can now analyze which implementation is better for which operations (if there are any differences). Let's now take a look at the runtime of each implementation.

Consider the `add(newEntry)` method for `ResizableArrayBag`. At first glance, the run-time appears to be $O(1)$ because you just go to the last location and insert there (all taking _constant time_). But what if the array is full? Well, we need to resize it. So, some adds are constant time ($O(1)$) while others take significantly more time, since we have to first allocate a new array and copy all of the data into it—taking linear time ($O(N)$). So, we would have $O(1) + O(N) = O(N)$.

Well, we have an operation that _sometimes_ takes $O(1)$ and sometimes takes $O(N)$. What we need to do is figure out the average time required over a sequence of operations. This is called **amortized analysis**. Although individual operations may vary in their run-time, we can get a consistent time for the overall sequence. Let's stick with the `add()` method for `ResizableArrayBag` and consider two different options for resizing: 1. Increase the array size by 1 each time we resize 2. Double the array size each time we resize (which is the implementation we chose earlier)

Let's take a look at the first option, where we increase the array size by 1 each time we resize. Note that with this approach, once we resize we will have to do it with every add. Thus rather than $O(1)$ our add() is now $O(N)$ all the time. 

To see why, assume the initial array is size 1: 
1. On first add, we just add the item (1 assignment) 
2. On second add, we allocate and assign 2 items
3. On third add, we allocate and assign 3 items 4. ...
On insert $i$, we allocate an _assign $i$ items_.

Overall, for $N$ `add()` operations look at the total number of assignments we have to make: $$1 + 2 + 3 + ... + N = N(N+1)/2 = O(N^2)$$Therefore, the **amortized add for one add operation** is $\frac{O(N^2)}{N} = O(N)$. This is linear and thus makes the overall `add()` operation for the `ResizableArrayBag` linear. We would like to do better than this.

So, we'll look at the second option, where we _double the array size at each resize_. Let's again assume that the initial array size is 1:
| Add Operation # | # of Assignments | Array Size at End of Operation |
| --------------- | ---------------- | ------------------------------ |
| 1               | 1                | 1                              |
| 2               | $2=1+2^0$        | 2                              |
| 3               | $3=1+2^1$        | 4                              |
| 4               | 1                | 4                              |
| 5               | $5=1+2^2$        | 8                              |
| ...             | 1                | 8                              |
| 9               | $9=1+2^3$        | 16                             |
| ...             | 1                | 16                             |
| 17              | $17=1+2^4$       | 32                             |
| ...             | 1                | 32                             |
| 32              | 1                | 32                             |

Note that every row has at least one assignment (for the new value being added). Some rows have more than one assignment (for copying the old array). For these additional assignments, notice that there is a pattern to their size. Rows that are $2^K + 1$ (for some positive integer $K$) have an additional $2^K$ assignments to copy data.

So, for $N$ adds, we have:
- $N$ assignments (for the actual data being added)
- $2^0 + 2^1 + 2^2 + ... + 2^x$ assignments (for the copying)
* Each term in that summation represents a time when the array is doubled. Notice that the array is doubled when: $$N = (\verb|array size at start of operation|) + 1$$Because the array sizes are always powers of 2, we can rewrite that as:$$N = 2^K + 1$$Solving for $K$, we get $$K = \log_2(N - 1)$$Now, this is for the add that doubles the array. But for determining the total number of assignments, we might not have added exactly enough to double the array (e.g. 13 adds). So we need to round up to get $x$:$$x = \verb|ceiling|[K] = \verb|ceiling|[\log_2(N - 1)]$$Now that we know what $x$ is, we now need to figure out the summation of $2^0 + ... + 2^x$: $$\verb|SUM|[i = 0\ to\ x]2^i =\sum_{i = 0}^{x}{2^i} = \sum_{i = 0}^{\log_2(N-1)}{2^i}$$This summation is the geometric series, so we can apply the [summation formula](https://en.wikipedia.org/wiki/Geometric_series#Formula) to get the result of the summation:$$\sum_{i = 0}^{\log_2(N - 1)}{2^i}= 2^{\log_2(N - 1)} - 1$$Finally, applying some simplifications, we arrive at the summation being $O(N)$:$$2^{\log_2(N - 1)} - 1 = N - 1 - 1 = O(N)$$
  So, for $N$ adds, we have:
* $N$ assignments (for the actual data being added)
* $2^0 + 2^1 + 2^2 + ... + 2^x = O(N)$ assignments (for the copying)
  Thus, the runtime for $N$ adds is $N + O(N) = O(N)$. So our amortized time for one add is $O(N) / N = O(1)$, which is **constant time**.

Recall that when increasing by 1 we had $O(N^2)$ overall for the sequence, which gives us $O(N)$ in amortized time. Note how much better our performance is when we double the array size!

Runtime analysis and amortized analysis can be complicated at times. Often, they'll have a good deal of math in it. That is what algorithm analysis is all about though. If you can do some math you can save yourself some programming.

## Run-time Analysis for LinkedBag
```Java
// LinkedBag
public boolean add (T newEntry)
{
    Node newNode = new Node(newEntry);
    newNode.next = firstNode;
    firstNode = newNode;
    
    numberofEntries++;
    
    return true;
}
```

What about the run-time for the singly linked list? Notice that this implementation always adds one to the size of the bag. Thus, clearly this is $O(1)$

### Other Bag operations

The text discusses other Bag operations. It turns out that for the Bag, the run-times for the array and the linked list are the same for every operation. This will not always be the case, as we'll see in the List ADT.


## Run-time Analysis for List
What are the Big-O complexities for our List implementations? What is different about the List from the Bag that may have an impact on the run-times? Let’s look at one operation in particular to highlight the difference: `getEntry(int i)`. This accesses an arbitrary location in the list and is not part of the `Bag`.

See [Example8.java](handout/Example8.java) and [Example9.java](handout/Example9.java)

Let’s compare our `AList<T>` and `LList<T>` implementations with regard to this operation

For the `AList<T>`, we simply index our array, and can access entry in: $O(1)$ time in every case. However, for `LList<T>`, it depends on the index. The sequential access requires us to traverse the list $i$ Nodes to get to $\verb|Node|_i$ . Thus, the `getEntry()` is $O(N)$ worst case for the `LinkedList`. Note that it could be less, depending on where the object is located. So maybe we should also consider the average case here, to be thorough.

To do this we need to make an assumption about the index chosen. Let's assume that all **index values are equally likely**. If this is not the case, we can still do the analysis, if we know the actual probability distribution for the index choice. Our assumption means that, given $N$ choices for an index, the probability of choosing a given index, $i$, (which we will call $P(i)$) is $\frac{1}{N}$ for any $i$. Let's define our key operation to be *"looking at*" a node in the list. So for a given index $i$, we will require $i$ operations to get to that location—we will call this value $\verb|Ops|(i)$.

We want to get the average (or, more correctly, the expected) case for the number of accesses required for a search. Each index contributes to this expected case with the following formula: $$\verb|Ops|(i) * P(i)$$The idea is that if location $i$ is chosen, we will need $\verb|Ops(i)|$ accesses, and the probability that $i$ is chosen is $P(i)$. For example, if $i = 5$ and we have $N = 20$, we would get: $5\cdot (\frac{1}{20}) = \frac{5}{20}$. Note that this is the contribution for a single index. So, we need to consider all index values.

Thus we define the average number of operations as:$$\overline{\verb|Ops|}=\sum_i(i*P(i))=\sum_i(i*\frac{1}{N})=\frac{1}{N}\cdot\sum_i(i)=\frac{1}{N}\cdot\frac{[N\cdot(N+1)]}{2}=\frac{(N+1)}{2}$$In an absolute sense, this is better than the worst case, but asymptotically it is the same. So in this case the worst and average cases for `getEntry()` on a linked list are the same, $O(N)$.
So we see: 
- `getEntry()` for array: $O(1)$
- `getEntry()` for linked list: $O(N)$

We will see later how [Iterators](08.%20Iterators.md) allow us improve our access for linked lists. Note also that this result does NOT mean that the `AList<T>` is always better than the `LList<T>`.
	For example, `add(1)` and `remove(1)` for the AList are $O(N)$. But `add(1)` and `remove(1)` for the LList are $O(1)$.
However, note that in the worst case `add(i)` and `remove(i)` are $O(N)$ for both the `AList` and `LList` for different reasons. To evaluate we need to assess which operations we need and how often we need them.

## Run-time Analysis for Queue
What about our Queue implementations? We have seen 3 so far:
1. keep front at index 0 and back will move  
2. keep back at index 0, and front will move
3. have front and back both move in cyclical way around the queue

(1) and (2) both require shifts (for different ops). For (3), none of the add and remove methods required shifting in this implementation.

Let’s consider option (1) and option (3) and consider the `enqueue()` and `dequeue()` operations.

Assume that everything we put into the queue we eventually take out. So we will consider the **amortized time** per op for $N$ `enqueue()` operations followed by $N$ `dequeue()` operations. So there are $2N$ total operations being done here. We will assume that resizing is $O(1)$ amortized time, as we previously showed. Thus we will not factor resizing into this analysis. Our key operation will here will be an assignment.

First we will consider the queue with front at 0. The $N$ `enqueue()` operations are clearly $O(1)$ each for a total of $O(N)$ for the $N$ operations.

For the $N$ `dequeue()` operations, we must:
1. Assign the return value (1 assignment each)
2. Shift the remaining items over to fill in the gap
So, 1st `dequeue()`: $N-1$ shifts + 1 return = $N$ total
2nd `dequeue()`  $N-2$ shifts + 1 return = $N-1$ total
3rd `dequeue()`  $N-3$ shifts + 1 return = $N-2$ total
...
$\therefore$ Total $= N + (N-1) + (N-2) + ... + 1 =\frac{N(N+1)}{2}$. 
This is $O(N^2)$ for all $2N$ operations. So, Amortized we get $O(N^2)/2N = O(N)$ per operation which is not good

Now consider our circular queue. The $N$ `enqueue()` operations are also $O(1)$ each for a total of $O(N)$ for the $N$ `enqueue()` operations. Now, however, the $N$ operations are also $O(1)$ each since we don't have to shift. This gives a total of $O(N)$ for the $N$ `dequeue()` operations. In total, we have $O(N) + O(N) = O(N)$. So, amortized per operation we have $O(N)/2N = O(1)$. Clearly this is a big improvement. Thus our analysis of these ADT implementations directs us to choose the circular array implementation. This is why we do asymptotic analysis


